{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "import os\n",
    "\n",
    "\n",
    "REPO_ID = \"PatronusAI/HaluBench\"\n",
    "FILE_NAME = \"data/test-00000-of-00001.parquet\"\n",
    "dataset_raw = pd.read_parquet(hf_hub_download(repo_id=REPO_ID, filename=FILE_NAME,repo_type=\"dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw[\"label\"] = dataset_raw[\"label\"].astype(str)\n",
    "dataset_raw[\"source_ds\"] = dataset_raw[\"source_ds\"].astype(str)\n",
    "#dataset_raw = dataset_raw[dataset_raw[\"source_ds\"] == \"FinanceBench\" ]\n",
    "\n",
    "dataset_raw = dataset_raw[dataset_raw[\"source_ds\"] == \"pubmedQA\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>passage</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "      <th>source_ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, passage, question, answer, label, source_ds]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw[dataset_raw[\"label\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ex =[dspy.Example({\"question\":e[\"question\"],\"context\":e[\"passage\"],\"gold_answer\":e[\"answer\"],\"id\":e[\"id\"],\"label\":e[\"label\"]}).with_inputs(\"question\",\"context\") for e in dataset_raw.to_dict('records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "   # \"\"\"Answer questions with one word answers.\"\"\" #used for Finance bench\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\" # used for PubMedQA\n",
    "\n",
    "    context = dspy.InputField(desc=\"contains relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self,question,context):\n",
    "        prediction = self.generate_answer(context=context , question = question)\n",
    "        return dspy.Prediction(context=context,answer= prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_data = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/200 [00:00<?, ?question/s]WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client AzureOpenAI, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 200/200 [00:00<00:00, 412.82question/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "azure_openai = dspy.AzureOpenAI(api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\") ,api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),api_version=\"2023-05-15\",model=\"chat-demo\")\n",
    "dspy.settings.configure(lm=azure_openai)\n",
    "\n",
    "rag = RAG()\n",
    "for e in tqdm(ds_ex[:len_of_data],desc=\"Processing questions\",unit=\"question\"):\n",
    "    pred = rag(question=e[\"question\"],context=e[\"context\"])\n",
    "    e[\"answer\"] = pred.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the data results for pubmed with GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data ={\n",
    "        'question':[],\n",
    "        'answer':[],\n",
    "        'contexts':[],\n",
    "        'ground_truth':[],\n",
    "        'label':[],\n",
    "        'id':[]\n",
    "    }\n",
    "\n",
    "for example in ds_ex[:len_of_data]:\n",
    "    # Append data to the respective lists\n",
    "    data['question'].append(example.question)\n",
    "    data['contexts'].append([example.context])  # Note: This is a list with one item\n",
    "    data['ground_truth'].append(example.gold_answer)\n",
    "    data['answer'].append(example[\"answer\"])\n",
    "    data['label'].append(example[\"label\"])\n",
    "    data['id'].append(example[\"id\"])\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('results/pubmed_results.json', 'w', encoding='utf-8') as f: #fs_results.json\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
