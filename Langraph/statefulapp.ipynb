{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class SomeState(TypedDict):\n",
    "    attribute1 : str\n",
    "    attribute2 : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function(state:SomeState) -> SomeState:\n",
    "    state['attribute1'] = 'value changed by some_function'\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.59-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 (from langgraph)\n",
      "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.43-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.1.145)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph)\n",
      "  Using cached msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.27.2)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
      "Requirement already satisfied: anyio in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.2.3)\n",
      "Downloading langgraph-0.2.59-py3-none-any.whl (135 kB)\n",
      "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
      "Downloading langgraph_checkpoint-2.0.8-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.1.43-py3-none-any.whl (31 kB)\n",
      "Using cached msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Installing collected packages: msgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.21\n",
      "    Uninstalling langchain-core-0.3.21:\n",
      "      Successfully uninstalled langchain-core-0.3.21\n",
      "Successfully installed langchain-core-0.3.24 langgraph-0.2.59 langgraph-checkpoint-2.0.8 langgraph-sdk-0.1.43 msgpack-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph \n",
    "\n",
    "graph = StateGraph(SomeState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x763e627f2630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node('node1',some_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x763e627f2630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "graph.add_edge('node1', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x763e627f2630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.set_entry_point('node1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(compiled_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(compiled_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attribute1': 'value changed by some_function', 'attribute2': 'value2'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_graph.invoke({'attribute1':'value1','attribute2':'value2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph,MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state :MessagesState)-> bool:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return 'tools'\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state:MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {'messages':messages+[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca7e1f38c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node('agent',call_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca7e1f38c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node('tools',tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca7e1f38c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START,'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca7e1f38c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_conditional_edges('agent',should_continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca7e1f38c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge('tools','agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(graph_compiled.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph_compiled.invoke({'messages':[('user','write a simple report to use AI for productivity')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='write a simple report to use AI for productivity', additional_kwargs={}, response_metadata={}, id='a9109fbc-445e-43a2-9511-d23533a46e56'),\n",
       "  AIMessage(content=\"# Report on Utilizing AI for Enhanced Productivity\\n\\n## Introduction\\n\\nIn today's fast-paced business environment, organizations are continually seeking ways to improve efficiency and productivity. Artificial Intelligence (AI) has emerged as a powerful tool that can streamline processes, enhance decision-making, and automate repetitive tasks. This report outlines the potential applications of AI to boost productivity across various sectors.\\n\\n## Benefits of AI for Productivity\\n\\n1. **Automation of Repetitive Tasks**: \\n   - AI can automate mundane tasks such as data entry, scheduling, and email management, allowing employees to focus on more strategic activities.\\n   - Tools like Robotic Process Automation (RPA) can handle high-volume, repetitive tasks with precision and speed.\\n\\n2. **Enhanced Decision-Making**:\\n   - AI algorithms can analyze vast amounts of data quickly, providing insights that help in making informed decisions.\\n   - Predictive analytics can forecast trends and outcomes, enabling proactive strategies.\\n\\n3. **Improved Customer Service**:\\n   - AI-powered chatbots and virtual assistants can handle customer inquiries 24/7, improving response times and customer satisfaction.\\n   - Natural Language Processing (NLP) can analyze customer feedback to identify areas for improvement.\\n\\n4. **Personalization**:\\n   - AI can analyze user behavior and preferences to deliver personalized experiences, whether in marketing, product recommendations, or customer interactions.\\n   - This tailored approach can lead to higher engagement and conversion rates.\\n\\n5. **Enhanced Collaboration**:\\n   - AI tools can facilitate better communication and collaboration among team members, regardless of their location.\\n   - Platforms that integrate AI can help manage projects, track progress, and streamline workflows.\\n\\n## Applications of AI in Various Sectors\\n\\n1. **Healthcare**:\\n   - AI can assist in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans.\\n   - Administrative tasks such as scheduling and billing can be automated, freeing up healthcare professionals to focus on patient care.\\n\\n2. **Finance**:\\n   - AI algorithms can detect fraudulent transactions, assess credit risk, and automate trading processes.\\n   - Financial institutions can use AI for customer service through chatbots and virtual advisors.\\n\\n3. **Manufacturing**:\\n   - AI can optimize supply chain management, predict equipment failures, and enhance quality control.\\n   - Robotics powered by AI can improve production efficiency and reduce human error.\\n\\n4. **Retail**:\\n   - AI can analyze shopping patterns to optimize inventory management and enhance the customer shopping experience.\\n   - Personalized marketing campaigns driven by AI can increase sales and customer loyalty.\\n\\n## Implementation Strategies\\n\\n1. **Identify Key Areas for AI Integration**:\\n   - Conduct a thorough analysis of existing processes to identify tasks that can benefit from AI automation.\\n\\n2. **Choose the Right Tools**:\\n   - Research and select AI tools that align with organizational goals and are user-friendly for employees.\\n\\n3. **Train Employees**:\\n   - Provide training sessions to ensure employees understand how to use AI tools effectively and can adapt to new workflows.\\n\\n4. **Monitor and Evaluate**:\\n   - Continuously monitor the impact of AI on productivity and make adjustments as necessary to optimize performance.\\n\\n## Conclusion\\n\\nThe integration of AI into business processes presents a significant opportunity to enhance productivity across various sectors. By automating repetitive tasks, improving decision-making, and personalizing customer interactions, organizations can achieve greater efficiency and effectiveness. As AI technology continues to evolve, embracing its potential will be crucial for staying competitive in the modern marketplace.\\n\\n## Recommendations\\n\\n- Organizations should start small by implementing AI in specific areas and gradually expand its use as they become more comfortable with the technology.\\n- Collaboration with AI experts or consultants can help in selecting the right tools and strategies for successful implementation.\\n- Regularly assess the impact of AI on productivity and be open to adapting strategies based on feedback and results. \\n\\n---\\n\\nThis report serves as a foundational guide for organizations looking to leverage AI to enhance productivity. By understanding the benefits and applications of AI, businesses can make informed decisions that drive growth and efficiency.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 799, 'prompt_tokens': 16, 'total_tokens': 815, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-8dcdad1d-5c66-4e69-9a29-0e6417a35f33-0', usage_metadata={'input_tokens': 16, 'output_tokens': 799, 'total_tokens': 815, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Report on Utilizing AI for Enhanced Productivity\n",
       "\n",
       "## Introduction\n",
       "\n",
       "In today's fast-paced business environment, organizations are continually seeking ways to improve efficiency and productivity. Artificial Intelligence (AI) has emerged as a powerful tool that can streamline processes, enhance decision-making, and automate repetitive tasks. This report outlines the potential applications of AI to boost productivity across various sectors.\n",
       "\n",
       "## Benefits of AI for Productivity\n",
       "\n",
       "1. **Automation of Repetitive Tasks**: \n",
       "   - AI can automate mundane tasks such as data entry, scheduling, and email management, allowing employees to focus on more strategic activities.\n",
       "   - Tools like Robotic Process Automation (RPA) can handle high-volume, repetitive tasks with precision and speed.\n",
       "\n",
       "2. **Enhanced Decision-Making**:\n",
       "   - AI algorithms can analyze vast amounts of data quickly, providing insights that help in making informed decisions.\n",
       "   - Predictive analytics can forecast trends and outcomes, enabling proactive strategies.\n",
       "\n",
       "3. **Improved Customer Service**:\n",
       "   - AI-powered chatbots and virtual assistants can handle customer inquiries 24/7, improving response times and customer satisfaction.\n",
       "   - Natural Language Processing (NLP) can analyze customer feedback to identify areas for improvement.\n",
       "\n",
       "4. **Personalization**:\n",
       "   - AI can analyze user behavior and preferences to deliver personalized experiences, whether in marketing, product recommendations, or customer interactions.\n",
       "   - This tailored approach can lead to higher engagement and conversion rates.\n",
       "\n",
       "5. **Enhanced Collaboration**:\n",
       "   - AI tools can facilitate better communication and collaboration among team members, regardless of their location.\n",
       "   - Platforms that integrate AI can help manage projects, track progress, and streamline workflows.\n",
       "\n",
       "## Applications of AI in Various Sectors\n",
       "\n",
       "1. **Healthcare**:\n",
       "   - AI can assist in diagnosing diseases, predicting patient outcomes, and personalizing treatment plans.\n",
       "   - Administrative tasks such as scheduling and billing can be automated, freeing up healthcare professionals to focus on patient care.\n",
       "\n",
       "2. **Finance**:\n",
       "   - AI algorithms can detect fraudulent transactions, assess credit risk, and automate trading processes.\n",
       "   - Financial institutions can use AI for customer service through chatbots and virtual advisors.\n",
       "\n",
       "3. **Manufacturing**:\n",
       "   - AI can optimize supply chain management, predict equipment failures, and enhance quality control.\n",
       "   - Robotics powered by AI can improve production efficiency and reduce human error.\n",
       "\n",
       "4. **Retail**:\n",
       "   - AI can analyze shopping patterns to optimize inventory management and enhance the customer shopping experience.\n",
       "   - Personalized marketing campaigns driven by AI can increase sales and customer loyalty.\n",
       "\n",
       "## Implementation Strategies\n",
       "\n",
       "1. **Identify Key Areas for AI Integration**:\n",
       "   - Conduct a thorough analysis of existing processes to identify tasks that can benefit from AI automation.\n",
       "\n",
       "2. **Choose the Right Tools**:\n",
       "   - Research and select AI tools that align with organizational goals and are user-friendly for employees.\n",
       "\n",
       "3. **Train Employees**:\n",
       "   - Provide training sessions to ensure employees understand how to use AI tools effectively and can adapt to new workflows.\n",
       "\n",
       "4. **Monitor and Evaluate**:\n",
       "   - Continuously monitor the impact of AI on productivity and make adjustments as necessary to optimize performance.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The integration of AI into business processes presents a significant opportunity to enhance productivity across various sectors. By automating repetitive tasks, improving decision-making, and personalizing customer interactions, organizations can achieve greater efficiency and effectiveness. As AI technology continues to evolve, embracing its potential will be crucial for staying competitive in the modern marketplace.\n",
       "\n",
       "## Recommendations\n",
       "\n",
       "- Organizations should start small by implementing AI in specific areas and gradually expand its use as they become more comfortable with the technology.\n",
       "- Collaboration with AI experts or consultants can help in selecting the right tools and strategies for successful implementation.\n",
       "- Regularly assess the impact of AI on productivity and be open to adapting strategies based on feedback and results. \n",
       "\n",
       "---\n",
       "\n",
       "This report serves as a foundational guide for organizations looking to leverage AI to enhance productivity. By understanding the benefits and applications of AI, businesses can make informed decisions that drive growth and efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(output['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (5.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (2.10.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.20.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (1.28.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (0.20.3)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (4.67.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (6.4.5)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.3)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/tash01-admin/reasearch/myvenv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Downloading onnxruntime-1.20.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading websockets-14.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=3b226c3a319cf9f4b8bb3f42af2b5351a58c6bb82472782070181072d0c3ce40\n",
      "  Stored in directory: /home/tash01-admin/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, websockets, uvloop, uvicorn, shellingham, pyproject_hooks, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, humanfriendly, httptools, grpcio, googleapis-common-protos, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, typer, opentelemetry-semantic-conventions, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.28.2\n",
      "    Uninstalling opentelemetry-api-1.28.2:\n",
      "      Successfully uninstalled opentelemetry-api-1.28.2\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.28.2\n",
      "    Uninstalling opentelemetry-sdk-1.28.2:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
      "Successfully installed asgiref-3.8.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 flatbuffers-24.3.25 googleapis-common-protos-1.66.0 grpcio-1.68.1 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 posthog-3.7.4 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 shellingham-1.5.4 starlette-0.41.3 typer-0.15.1 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 0}, page_content='Large Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3 Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC. This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC.\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗ Equal Contribution. The order is determined by dice\\nrolling.\\n† Corresponding Authors.\\nAgentHuman Env\\nAllocator Env\\nEnv\\n(a)\\nActAct\\n(b)\\n(c)\\nAgent\\nHuman\\nAct\\nAct\\nI need help\\nFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-\\narXiv:2402.12914v1  [cs.CL]  20 Feb 2024')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'assets_pdf/human-agent-collab-problem-solving.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 0}, page_content='Large Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3 Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC. This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC.\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗ Equal Contribution. The order is determined by dice\\nrolling.\\n† Corresponding Authors.\\nAgentHuman Env\\nAllocator Env\\nEnv\\n(a)\\nActAct\\n(b)\\n(c)\\nAgent\\nHuman\\nAct\\nAct\\nI need help\\nFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-\\narXiv:2402.12914v1  [cs.CL]  20 Feb 2024'),\n",
       " Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 1}, page_content='narios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving, aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts and\\nlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC, aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\nachieving a balance between effectiveness and effi-\\nciency.\\n2 Approach\\nIn this section, we first formulate the problem\\nof human-agent collaboration for complex task\\nsolving, and then introduce our proposed ReHAC\\nmethod in detail.\\n2.1 Preliminary and Problem Formulation\\nComplex task-solving, inherently necessitating\\nmulti-step planning and reasoning, is convention-\\nally formalized as a multi-step decision-making\\nproblem. Historically, complex task-solving was\\npredominantly achieved through human-driven\\nmethods. These methods leveraged human cogni-\\ntive capabilities to determine the suitable action in\\neach step. Formally, considering a complex task\\nq, it is traditionally solved via a sequence of ac-\\ntions (a1, a2, ··· an), with each action determined'),\n",
       " Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 2}, page_content='by human decision-making, expressed as:\\nat = Human(q, st), (1)\\nwhere st = (a1, o1, ··· , at−1, ot−1) denotes the\\nhistory information of task state at step t and ot is\\nthe observation after at−1 is proceeded.\\nThe advent of LLMs has brought a paradigm\\nshift in this arena. Their impressive understand-\\ning and reasoning abilities have prompted research\\ninto LLM-based agents for complex task-solving,\\nthereby enhancing the level of automation in task-\\nsolving. These agent-driven methods (e.g., Re-\\nAct (Yao et al., 2022)), leverage LLM-based agents\\nto supplant human decision-making. This shift is\\nrepresented as:\\nat = Agent(q, st). (2)\\nThis evolution of such AI-driven techniques pro-\\nvides a way to the automation of complex task-\\nsolving.\\nHowever, limited by the current intelligence\\nlevel of LLMs, full automation based on agent-\\ndriven methods is not yet feasible in practical sce-\\nnarios (Kiseleva et al., 2022; Mehta et al., 2023).\\nInspired by autonomous driving (Cui et al., 2024;\\nFu et al., 2024; Bastola et al., 2024), we propose\\nthe problem of LLM-based human-agent collab-\\noration for complex task solving and explore the\\ndynamics and efficacy of the human-agent collab-\\norative methods for complex task solving. We\\nfirst explore a specific form of human-agent col-\\nlaboration: humans intervene in the complex task-\\nsolving process when necessary. Formally, we need\\nto determine whether a human or an agent makes\\ndecisions based on the actions’ complexity and\\ncontextual changes, i.e.,\\nat = Human(q, st) or Agent (q, st), (3)\\nIt is generally perceived that direct human in-\\ntervention in decision-making, particularly in real-\\nworld scenarios, incurs higher costs and diminishes\\nthe system’s automation level (Cai et al., 2023;\\nWang et al., 2023b). On the other hand, human\\nintervention plays an important role in enhancing\\ntask performance and flexibility. Therefore, the\\nobjective of human-agent collaboration is to en-\\nhance the effectiveness of complex task-solving\\nwith minimal reliance on human decision-making.\\nOne key challenge is to determine the stages in\\nthe task-solving process where human interven-\\ntion is most beneficial and effective, aligning\\nwith the goal of minimizing human involvement\\nwhile maximizing task performance.\\n2.2 ReHAC\\nIn this work, we propose a Reinforcement learning-\\nbased Human-Agent Collaboration method, Re-\\nHAC. It formulates the human-agent collabo-\\nration problem as a Markov Decision Process\\n(MDP) framework, represented by the tuple\\n(S, A, P, R, γ), where S is the set of states, A is\\nthe set of actions, P : S × A ×S is the state transi-\\ntion probabilities, R serves as the reward function,\\nand γ the discount factor.\\nFor each action at ∈ A, we define it as a tuple\\n(acollab\\nt , atask\\nt ), where acollab\\nt indicates the subtask\\nis allocated to an agent or a human, andatask\\nt is the\\ntask action determined by agent or human:\\nacollab\\nt ∼ πcollab\\nθ1 (acollab\\nt |st)\\natask\\nt ∼\\n(\\nπtask\\nθ2 (atask\\nt |st), if acollab\\nt = 0;\\nπtask\\nHuman(atask\\nt |st), otherwise,\\n(4)\\nwhere πcollab\\nθ1 is the collaboration policy model,\\nπtask\\nθ2 is the agent-based task policy model, and\\nπtask\\nHuman is the human task policy.\\nTo balance the maximization of task perfor-\\nmance and the cost of human intervention, we de-\\nfine the reward function as:\\nR(s, a) =T(s, a) − λC(s, a), (5)\\nwhere T(s, a) is the measure of expected task re-\\nwards received after taking action a in state s,\\nC(s, a) is the number of human interventions in\\nthe trajectory after taking action a, λ is a hyper-\\nparameter that serves as a penalty coefficient of the\\nnumber of human interventions. We utilize Monte-\\nCarlo estimation to compute this reward function.\\nOptimization: Following the REINFORCE algo-\\nrithm (Williams, 1992), we optimize the expected\\nreward:\\nJ (πθ) =Eπθ [R(s, a)], (6)\\nwhich aims to find an optimal policyπθ that ensures\\nthe maximization of task rewards while minimizing\\nthe human intervention costs, and θ = [θ1, θ2].\\nWe utilize the advantage function to enhance the\\nstability of optimization and important sampling'),\n",
       " Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 3}, page_content='for offline learning:\\nA(s, a) =R(s, a) − 1\\n|A|\\nX\\na′∈A\\nR(s, a′)\\n∇θJ (πθ) =\\nX\\ns\\nX\\na\\nw(s, a)∇θ log πθ(a|s)A(s, a),\\nw(h, a) =Clip\\n\\x12 πθ(s, a)\\nπbeh(s, a)\\n\\x13\\n, (7)\\nwhere A(s, a) is the advantage function, the clip\\nfunction limits the importance sampling term to the\\ninterval 1 − ϵ to 1 +ϵ, and the behavior policyπbeh\\nrepresents the policy under of the offline training.\\nMoreover, we have incorporated an entropy regu-\\nlarization term. This term encourages the policy\\nto explore a variety of actions, thereby preventing\\nthe policy from becoming too deterministic and\\noverfitting to the training data. Finally, the gradient\\nof objective function is as follows:\\n∇θ ˜J (πθ) =∇θJ (πθ) +α∇θH(πθ(·|s)). (8)\\n3 Experiments\\n3.1 Experimental Setup\\nDatasets Following Yao et al. (2022); Shinn et al.\\n(2023); Liu et al. (2023b); Xu et al. (2023), we eval-\\nuate the efficacy of our method on question answer-\\ning and coding datasets: (1) HotpotQA (Yang et al.,\\n2018) is a Wikipedia-based question answering\\nbenchmark which needs model to perform multi-\\nhop reasoning over complex questions. (2) Strate-\\ngyQA (Geva et al., 2021) is a question answering\\nbenchmark with questions that need implicit rea-\\nsoning. (3) InterCode (Yang et al., 2023) is an\\ninteractive coding dataset that enables agents to\\nreceive feedback from the code interpreter. In this\\nwork, we use InterCode-SQL part, which requires\\nmodels to write SQL statements to fulfil the query.\\nImplementation details We use LLaMA-2 (Tou-\\nvron et al., 2023) as the collaboration policy model\\nπcollab\\nθ1 and use Low-Rank Adaptation (LoRA, Hu\\net al. (2021)) methods to train the policy model.\\nIn all experiments, we utilized ChatGPT (gpt-3.5-\\nturbo-0613) to simulate the agent policy πtask\\nθ2 .\\nMore model implementation and data collection\\ndetails can be found in Appendix A.1.\\nIn this study, we set humans and agents to solve\\ntasks under the ReAct framework (Yao et al., 2022)\\nfor question-answering datasets. The action space\\nof atask is {Search[entity], Lookup[keyword], and\\nFinish[answer]}. All actions are supported by a\\nWikipedia web API, following the original Re-\\nAct implementation. For the InterCode dataset,\\nwe solve tasks under the “Try Again” framework\\n(Yang et al., 2023). Here, agents and humans in-\\nteract with the code interpreter through the action\\nat and receive execution outputs from the code\\ninterpreter as observations ot. The task-solving\\nprocess ends if any one of the following conditions\\nis satisfied: 1) the Finish[answer] action is exe-\\ncuted actively by πtask\\nθ2 for the question answering\\ndataset. 2) the task reward T(s, a) = 1for Inter-\\nCode dataset. 3) the number of actions t exceeds a\\npre-defined step threshold.\\nReward Calculation For all datasets, the final\\nreward is computed as equation (5). For question\\nanswering datasets, we choose the F1 score as the\\ntask reward T(s, a). For the InterCode dataset,\\nfollowing Yang et al. (2023), we use Intersection\\nover Union as the task reward T(s, a).\\nBaselines We compare our method ReHAC with\\nthe following baselines: 1) Agent-only which car-\\nries out all actions by agents. 2) Human-only,\\nwhich conducts all actions by humans. 3) Ran-\\ndom, which selects an agent or human randomly\\nat a probability of 50% to perform each action. 4)\\nPrompt, which prompts the agent to actively decide\\nwhether the action is executed by itself or a human.\\n5) Imitation Learning (IL), which trains the pol-\\nicy model to decide whether the action should be\\nfinished by an agent or human by the IL method.\\nMore details about baselines can be found in the\\nAppendix A.2.\\n3.2 Overall Results\\nIn this section, we verify the effectiveness of our\\nproposed ReHAC method for human-agent collab-\\noration on the HotpotQA dataset.\\nHuman-Agent Experiments Figure 2(a) shows\\nthe evaluation results of human-agent collabora-\\ntion on the HotpotQA dataset. From the figure,\\nwe can observe that all human-agent collabora-\\ntion methods outperform Human-only and Agent-\\nonly methods. This underscores the importance\\nof collaborating human and agent in complex\\ntask-solving for getting higher reward. In addi-\\ntion, ReHACHuman achieves the best performance\\ncompared with prompt-based and random-based\\nmethod in achieving higher rewards. Specifically,\\nwhen λ = 0.06, ReHAC achieves a higher reward\\nwith approximately 30% more human interventions'),\n",
       " Document(metadata={'source': 'assets_pdf/human-agent-collab-problem-solving.pdf', 'page': 4}, page_content='ReHACGPT 4\\n ReHACHuman Prompt Random Human-only Agent-only\\n=0.06\\n =0.08\\n =0.1\\n40\\n30\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\nReward Values\\n78.96\\n52.66\\n27.11\\n78.35\\n45.18\\n24.93\\n47.20\\n47.20\\n47.20\\n50.44\\n50.44\\n50.44\\n100\\n100\\n100\\n0\\n0\\n0\\n(a)\\n=0.06\\n =0.08\\n =0.1\\n40\\n30\\n20\\n10\\n0\\n10\\n20\\n30\\n40\\nReward Values\\n79.13\\n51.46\\n27.37\\n78.97\\n42.18\\n24.27\\n34.06\\n34.06\\n34.06\\n50.84\\n50.84\\n50.84\\n100\\n100\\n100\\n0\\n0\\n0\\n(b)\\nFigure 2: (a) Human-agent collaboration evaluation. (b) GPT-4-agent collaboration evaluation. The bars above\\nthe 0-axis represent the reward R, the bars below the 0-axis represent the human intervention cost λC, and the\\nentire columns, composed of the bars above and below the 0-axis, represent the task reward T. Numbers within\\nthe bars means the human intervention rate (%). ReHACGPT-4 and ReHACHuman represent the policy model trained\\non GPT-4-agent and human-agent collaboration datasets, respectively. ReHAC outperforms other baselines in\\nhuman-agent collaboration scenarios.\\ncompared with the prompt-based baseline; when\\nλ = 0.1, it also achieves a reward improvement\\nwith about 20% less human interventions. This\\nindicates that our ReHAC method can dynamically\\nintroduce human intervention in real human-agent\\ncollaboration scenarios, thereby achieving a bal-\\nance between effectiveness and efficiency.\\nFocusing on ReHACHuman, we observe that as\\nλ increases, the human intervention rate 1 (HIR)\\nof ReHACHuman gradually decreases. This trend\\nsuggests that a higher human penalty coefficient el-\\nevates our policy model’s “threshold” for assigning\\nactions to humans. Simultaneously, the decrease of\\nthe HIR correspondingly results in a deterioration\\nof human-agent interaction performance.\\nHuman Simulation Due to the high cost of hir-\\ning annotators to label real human-agent collabora-\\ntion data, it is costly for us to collect human-agent\\ncollaboration data on more datasets and, as a result,\\nvalidate the efficacy of our method in broader sce-\\nnarios. We instead use GPT-4 (gpt-4-0613) to build\\na simulation environment and make it collaborate\\nwith agents to solve tasks. This setup enables us to\\ncollect more “human-agent” collaboration data at a\\nreasonable cost.\\nTo verify the feasibility of using GPT-4 to simu-\\nlate humans to collect “human-agent” collaboration\\n1The formula for calculating the human intervention rate\\nis in Appendix A.3.\\ndata, we learn ReHAC on the HotpotQA GPT-4-\\nagent collaboration data, named as ReHACGPT-4\\nand test its performance in the real human-agent\\ncollaboration environment. From Figure 2(a), we\\ncan see that ReHACGPT-4 exhibits better perfor-\\nmance compared to ReHACHuman in human-agent\\ncollaboration when λ = 0.06 and 0.08. We sup-\\npose that this is possibly attributed to individual\\ndifferences among humans, leading to a distribu-\\ntion variance in the human-agent collaboration\\ndata, while GPT-4-agent collaboration data exhibits\\nhigher consistency and lower variance. This makes\\nReHACGPT-4 learn the collaboration signal more\\neasily, and thus is more stable and performs better.\\nTo further reduce costs and observe the reward\\nvariation of ReHAC during the training process,\\nwe use GPT-4 to simulate humans in the evalua-\\ntion phase. Figure 2(b) shows the evaluation re-\\nsults when using GPT-4 to simulate humans for\\ncollaboration. Comparing the results in Figure 2(a)\\nand (b), we notice that the relative performance\\nof various methods is generally consistent in both\\nhuman-agent collaboration and GPT-4-agent col-\\nlaboration. For example, the rewards R of ReHAC\\nconsistently surpass those of the Prompt method,\\nand both ReHAC and the Prompt method outper-\\nform the Random method. This demonstrates the\\nviability of using GPT-4 to simulate humans for\\nevaluation.\\nConsidering feasibility and cost-effectiveness,')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents = docs , collection_name = 'rag-chroma',embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(retriever,'retrive_info_from_paper','Search and retriver information about a paper')\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state:AgentState):\n",
    "    messages = state['messages']\n",
    "    model = ChatOpenAI(model='gpt-4o-mini',temperature=0)\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return {'messages':[response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca5b68e900>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,MessagesState,START,END\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node('agent',agent)\n",
    "retriever_node = ToolNode([retriever_tool])\n",
    "graph.add_node('retrieve',retriever_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x76ca5b68e900>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START,'agent')\n",
    "graph.add_conditional_edges('agent',tools_condition,{'tools':'retrieve',END:END})\n",
    "graph.add_edge('retrieve','agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_compiled = graph.compile()\n",
    "Image(graph_compiled.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "inputs ={\"messages\":[(\"user\",\"in this paper how do authors setup collboration between human and the LLM?\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2fMqc47UxuhM5Kh6V8i7wYxQ', 'function': {'arguments': '{\"query\":\"collaboration between human and LLM\"}', 'name': 'retrive_info_from_paper'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 76, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_818c284075', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f0beb9b9-37c8-4ea1-ae39-52927d9c5869-0', tool_calls=[{'name': 'retrive_info_from_paper', 'args': {'query': 'collaboration between human and LLM'}, 'id': 'call_2fMqc47UxuhM5Kh6V8i7wYxQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 76, 'output_tokens': 25, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "'\\n---\\n'\n",
      "\"output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='Large Language Model-based Human-Agent Collaboration\\nfor Complex Task Solving\\nXueyang Feng1,2∗, Zhi-Yuan Chen1,2∗, Yujia Qin3, Yankai Lin1,2†\\nXu Chen1,2†, Zhiyuan Liu3, Ji-Rong Wen1,2\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2 Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\\n3 Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n{xueyangfeng, zhiyuanc2001, yankailin, xu.chen}@ruc.edu.cn\\nAbstract\\nIn recent developments within the research\\ncommunity, the integration of Large Language\\nModels (LLMs) in creating fully autonomous\\nagents has garnered significant interest. De-\\nspite this, LLM-based agents frequently demon-\\nstrate notable shortcomings in adjusting to dy-\\nnamic environments and fully grasping hu-\\nman needs. In this work, we introduce the\\nproblem of LLM-based human-agent collab-\\noration for complex task-solving, exploring\\ntheir synergistic potential. In addition, we pro-\\npose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC. This\\napproach includes a policy model designed to\\ndetermine the most opportune stages for hu-\\nman intervention within the task-solving pro-\\ncess. We construct a human-agent collabo-\\nration dataset to train this policy model in\\nan offline reinforcement learning environment.\\nOur validation tests confirm the model’s ef-\\nfectiveness. The results demonstrate that the\\nsynergistic efforts of humans and LLM-based\\nagents significantly improve performance in\\ncomplex tasks, primarily through well-planned,\\nlimited human intervention. Datasets and code\\nare available at: https://github.com/\\nXueyangFeng/ReHAC.\\n1 Introduction\\nIn today’s increasingly complex world, humans are\\nconfronted with multifaceted tasks stemming from\\ntechnical, social, and economic domains. Solv-\\ning these complex tasks necessitates not only hu-\\nman interaction with the environment but also in-\\ntricate decision-making processes. To alleviate\\nhuman workload and enhance the automation of\\ntasks in both professional and personal spheres, re-\\nsearchers have been actively developing advanced\\ntools for human assistance (Zawacki-Richter et al.,\\n∗ Equal Contribution. The order is determined by dice\\nrolling.\\n† Corresponding Authors.\\nAgentHuman Env\\nAllocator Env\\nEnv\\n(a)\\nActAct\\n(b)\\n(c)\\nAgent\\nHuman\\nAct\\nAct\\nI need help\\nFigure 1: Different Levels of Automation. (a) No au-\\ntomation: Tasks are entirely performed by humans. (b)\\nFull automation: Tasks are completely executed by\\nagents without human intervention. (c) Conditional\\nautomation: Humans are required only for specific sub-\\ntasks, without continuous monitoring.\\n2019; Amershi et al., 2019). Recently, the emer-\\ngence of Large Language Models (LLMs) such\\nas LLaMA (Touvron et al., 2023), Gemini (Team\\net al., 2023) and GPT (Brown et al., 2020; Achiam\\net al., 2023) has marked a significant milestone.\\nLLMs’ remarkable abilities in task understanding,\\nplanning, and reasoning (Zhao et al., 2023b) have\\ngiven rise to the development of LLM-based au-\\ntonomous agents (Wang et al., 2023a; Yao et al.,\\n2022; Shinn et al., 2023). These agents are de-\\nsigned to leverage the LLMs’ capabilities to assist\\nhumans in solving complex tasks autonomously.\\nThe LLMs’ capabilities enable them to effectively\\nnavigate and address the complexities encountered\\nin real-world scenarios, thereby offering substan-\\ntial support in human decision-making processes\\nof task-solving.\\nDespite the remarkable progress of LLM-based\\nagents, there remains a notable gap in their intelli-\\ngence level to handle complex and dynamic real-\\nworld tasks with human-like proficiency. This limi-\\ntation poses a significant challenge to their practi-\\ncality in real-world applications, especially in sce-\\narXiv:2402.12914v1  [cs.CL]  20 Feb 2024\\n\\nnarios where high accuracy is crucial, such as the\\nlegal or financial domains. Addressing this chal-\\nlenge extends beyond just enhancing the agents’\\ncapabilities. Incorporating human intuition and\\nwisdom is equally vital for the effective manage-\\nment of these intricate and evolving tasks, offering\\na complementary approach to the limitations of\\ncurrent agent technologies.\\nIn this work, we introduce the problem of LLM-\\nbased human-agent collaboration for complex\\ntask solving, aiming to augment the capabilities of\\nLLM-based agents by integrating human intuition\\nand wisdom. The idea is analogous to the evolu-\\ntion in autonomous driving technology, which has\\nbeen categorized into varying levels of autonomy,\\nranging from no automation, conditional automa-\\ntion to full automation (Khan et al., 2022; SAE\\nInternational, 2021). Referring to this framework,\\nwe define the different levels of human-agent col-\\nlaboration, as illustrated in Figure 1. Applying\\nthis conditional automation mode to LLM-based\\nagents offers a practical path for their deployment\\nin real-world scenarios, acknowledging the current\\nlimitations in their cognitive capabilities. Instead\\nof aiming for full automation, human-agent col-\\nlaboration under the paradigm of conditional au-\\ntomation enables humans to intervene the complex\\ntask-solving when necessary, while agents handle\\nmost of the sub-tasks. This takes advantage of both\\nhuman and machine intelligence.\\nWhile advancements in LLMs significantly en-\\nhance the capacity for mutual understanding in\\nhuman-agent collaboration, several crucial chal-\\nlenges persist. These challenges include defining\\nthe division of labor between humans and agents,\\ndetermining the granularity of tool execution, man-\\naging proactive interruption, and implementing\\nmulti-level intervention. However, our research\\nspecifically focuses on scenarios where humans\\ndirectly replace agents in action. The key chal-\\nlenge we aim to address in human-agent collab-\\noration lies in determining the optimal stages for\\nhuman intervention in task-solving and minimiz-\\ning such intervention to enhance efficiency. Some\\nresearchers have made preliminary attempts, by de-\\nsigning heuristic rules or specialized prompts to\\ndetermine the stages at which agents should seek\\nhuman assistance (Cai et al., 2023; Wu et al., 2022a;\\nMehta et al., 2023; Wang et al., 2023b). However,\\nthese rule-based or prompt-driven approaches are\\nheavily reliant on specific application contexts and\\nlack universality. They often demand a deep under-\\nstanding of the domain and substantial experience\\nfrom the designers, otherwise, suboptimal design\\nchoices can lead to reduced performance. Apart\\nfrom that, a standardized formal framework and\\nuniversally accepted paradigm for leveraging large\\nlanguage models (LLMs) in human-agent collabo-\\nration is still lacking.\\nTo overcome the aforementioned challenges, we\\npropose a Reinforcement Learning-based Human-\\nAgent Collaboration method, ReHAC, aimed at\\neffectively combining human intervention with the\\nautomation capabilities of LLM-based agents. Our\\nmethod, leveraging reinforcement learning, trains\\na policy model to dynamically identify the most\\nadvantageous moments for human input during the\\ntask-solving process. ReHAC is a learnable gen-\\neral framework that can be applied to various sce-\\nnarios and does not require additional prior knowl-\\nedge to design rules and prompts. For training\\nthis policy model, we collect a dataset compris-\\ning tasks collaboratively completed by humans and\\nLLM-based agents, utilized for the offline training\\nof the policy model. We conducted extensive ex-\\nperiments on three multi-step reasoning datasets:\\nHotpotQA, StrategyQA, and InterCode, using two\\npopular LLM-based agent frameworks, ReAct and\\n\"Try-again\". The experimental results indicate that\\nwith a policy model learned from limited data, Re-\\nHAC can effectively allocate human intervention\\nin human-agent collaboration scenarios, thereby\\nachieving a balance between effectiveness and effi-\\nciency.\\n2 Approach\\nIn this section, we first formulate the problem\\nof human-agent collaboration for complex task\\nsolving, and then introduce our proposed ReHAC\\nmethod in detail.\\n2.1 Preliminary and Problem Formulation\\nComplex task-solving, inherently necessitating\\nmulti-step planning and reasoning, is convention-\\nally formalized as a multi-step decision-making\\nproblem. Historically, complex task-solving was\\npredominantly achieved through human-driven\\nmethods. These methods leveraged human cogni-\\ntive capabilities to determine the suitable action in\\neach step. Formally, considering a complex task\\nq, it is traditionally solved via a sequence of ac-\\ntions (a1, a2, ··· an), with each action determined\\n\\nby human decision-making, expressed as:\\nat = Human(q, st), (1)\\nwhere st = (a1, o1, ··· , at−1, ot−1) denotes the\\nhistory information of task state at step t and ot is\\nthe observation after at−1 is proceeded.\\nThe advent of LLMs has brought a paradigm\\nshift in this arena. Their impressive understand-\\ning and reasoning abilities have prompted research\\ninto LLM-based agents for complex task-solving,\\nthereby enhancing the level of automation in task-\\nsolving. These agent-driven methods (e.g., Re-\\nAct (Yao et al., 2022)), leverage LLM-based agents\\nto supplant human decision-making. This shift is\\nrepresented as:\\nat = Agent(q, st). (2)\\nThis evolution of such AI-driven techniques pro-\\nvides a way to the automation of complex task-\\nsolving.\\nHowever, limited by the current intelligence\\nlevel of LLMs, full automation based on agent-\\ndriven methods is not yet feasible in practical sce-\\nnarios (Kiseleva et al., 2022; Mehta et al., 2023).\\nInspired by autonomous driving (Cui et al., 2024;\\nFu et al., 2024; Bastola et al., 2024), we propose\\nthe problem of LLM-based human-agent collab-\\noration for complex task solving and explore the\\ndynamics and efficacy of the human-agent collab-\\norative methods for complex task solving. We\\nfirst explore a specific form of human-agent col-\\nlaboration: humans intervene in the complex task-\\nsolving process when necessary. Formally, we need\\nto determine whether a human or an agent makes\\ndecisions based on the actions’ complexity and\\ncontextual changes, i.e.,\\nat = Human(q, st) or Agent (q, st), (3)\\nIt is generally perceived that direct human in-\\ntervention in decision-making, particularly in real-\\nworld scenarios, incurs higher costs and diminishes\\nthe system’s automation level (Cai et al., 2023;\\nWang et al., 2023b). On the other hand, human\\nintervention plays an important role in enhancing\\ntask performance and flexibility. Therefore, the\\nobjective of human-agent collaboration is to en-\\nhance the effectiveness of complex task-solving\\nwith minimal reliance on human decision-making.\\nOne key challenge is to determine the stages in\\nthe task-solving process where human interven-\\ntion is most beneficial and effective, aligning\\nwith the goal of minimizing human involvement\\nwhile maximizing task performance.\\n2.2 ReHAC\\nIn this work, we propose a Reinforcement learning-\\nbased Human-Agent Collaboration method, Re-\\nHAC. It formulates the human-agent collabo-\\nration problem as a Markov Decision Process\\n(MDP) framework, represented by the tuple\\n(S, A, P, R, γ), where S is the set of states, A is\\nthe set of actions, P : S × A ×S is the state transi-\\ntion probabilities, R serves as the reward function,\\nand γ the discount factor.\\nFor each action at ∈ A, we define it as a tuple\\n(acollab\\nt , atask\\nt ), where acollab\\nt indicates the subtask\\nis allocated to an agent or a human, andatask\\nt is the\\ntask action determined by agent or human:\\nacollab\\nt ∼ πcollab\\nθ1 (acollab\\nt |st)\\natask\\nt ∼\\n(\\nπtask\\nθ2 (atask\\nt |st), if acollab\\nt = 0;\\nπtask\\nHuman(atask\\nt |st), otherwise,\\n(4)\\nwhere πcollab\\nθ1 is the collaboration policy model,\\nπtask\\nθ2 is the agent-based task policy model, and\\nπtask\\nHuman is the human task policy.\\nTo balance the maximization of task perfor-\\nmance and the cost of human intervention, we de-\\nfine the reward function as:\\nR(s, a) =T(s, a) − λC(s, a), (5)\\nwhere T(s, a) is the measure of expected task re-\\nwards received after taking action a in state s,\\nC(s, a) is the number of human interventions in\\nthe trajectory after taking action a, λ is a hyper-\\nparameter that serves as a penalty coefficient of the\\nnumber of human interventions. We utilize Monte-\\nCarlo estimation to compute this reward function.\\nOptimization: Following the REINFORCE algo-\\nrithm (Williams, 1992), we optimize the expected\\nreward:\\nJ (πθ) =Eπθ [R(s, a)], (6)\\nwhich aims to find an optimal policyπθ that ensures\\nthe maximization of task rewards while minimizing\\nthe human intervention costs, and θ = [θ1, θ2].\\nWe utilize the advantage function to enhance the\\nstability of optimization and important sampling\\n\\nto pinpoint the most critical junctures for human\\nintervention within the task-solving trajectory. Our\\nexperimental results show that ReHAC aspects bet-\\nter results and is more generalizable than heuristic\\nrule-based or prompt-based approaches in human-\\nagent collaboration tasks. We believe that ReHAC\\noffers a practical pathway for the application of\\nllm-agents in real-world scenarios.\\nEthical Considerations and Limitations\\nThe objective of this work focuses on human-agent\\ncollaboration, which requires humans to interact\\nwith LLM-based agents. We acknowledge that\\nagents are likely to output some hallucinations and\\nmisleading information, and it is unclear how these\\ncontents impact humans. Additionally, all datasets\\nused in this work are publicly available, and there-\\nfore, there are no data privacy concerns. All data\\ncollected will be used for research purposes only\\nThe limitations of this paper can be summarised\\nin three aspects:\\n1) The current study is confined to basic LLM-\\nbased agent architectures based on the \"ReAct\"\\nand \"Try Again\" frameworks, while more complex\\narchitectures involving self-reflection and memory\\ncapabilities are still unexplored.\\n2) Our research primarily focuses on the use of\\n7B and 13B scale models as policy models for task\\nallocation. Future work will investigate the feasi-\\nbility of smaller models in carrying out these tasks,\\naiming to maintain performance while reducing\\nresource consumption.\\n3) This study is based on the assumption that hu-\\nman performance supersedes that of agents. How-\\never, as technology advances, agents might surpass\\nhuman capabilities. Future research will thus shift\\ntowards exploring human-agent collaboration mod-\\nels in this new context. Emphasis will be placed\\non assessing how human-agent collaboration can\\nensure the safety of agent decisions while aligning\\nwith human preferences.\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\\narXiv preprint arXiv:2303.08774.\\nSaleema Amershi, Dan Weld, Mihaela V orvoreanu,\\nAdam Fourney, Besmira Nushi, Penny Collisson, Jina\\nSuh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al.\\n2019. Guidelines for human-ai interaction. In Pro-\\nceedings of the 2019 chi conference on human factors\\nin computing systems, pages 1–13.\\nAshish Bastola, Julian Brinkley, Hao Wang, and Abol-\\nfazl Razi. 2024. Driving towards inclusion: Revis-\\niting in-vehicle interaction in autonomous vehicles.\\narXiv preprint arXiv:2401.14571.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nZefan Cai, Baobao Chang, and Wenjuan Han. 2023.\\nHuman-in-the-loop through chain-of-thought. arXiv\\npreprint arXiv:2306.07932.\\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu,\\nYantao Li, Jianbing Zhang, and Zhiyong Wu. 2024.\\nSeeclick: Harnessing gui grounding for advanced\\nvisual gui agents. arXiv preprint arXiv:2401.10935.\\nCan Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran\\nWang. 2024. Drive as you speak: Enabling human-\\nlike interaction with large language models in au-\\ntonomous vehicles. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vi-\\nsion, pages 902–909.\\nYang Deng, Wenxuan Zhang, Wai Lam, See-Kiong\\nNg, and Tat-Seng Chua. 2023. Plug-and-play policy\\nplanner for large language model powered dialogue\\nagents. arXiv preprint arXiv:2311.00262.\\nDaocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong\\nCai, Botian Shi, and Yu Qiao. 2024. Drive like a\\nhuman: Rethinking autonomous driving with large\\nlanguage models. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vi-\\nsion, pages 910–919.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,\\nZhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023.\\nReasoning with language model is planning with\\nworld model. In Conference on Empirical Methods\\nin Natural Language Processing.\\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu,\\nYong Dai, Hongming Zhang, Zhenzhong Lan, and\\nDong Yu. 2024. Webvoyager: Building an end-to-\\nend web agent with large multimodal models. arXiv\\npreprint arXiv:2401.13919.\\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng\\nCheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023.', name='retrive_info_from_paper', id='24142367-c74f-4f46-9308-3fb36c005893', tool_call_id='call_2fMqc47UxuhM5Kh6V8i7wYxQ')]}\n",
      "'\\n---\\n'\n",
      "\"output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='In the paper titled \"Large Language Model-based Human-Agent Collaboration for Complex Task Solving,\" the authors set up collaboration between humans and Large Language Models (LLMs) through a method called Reinforcement Learning-based Human-Agent Collaboration (ReHAC). Here are the key points regarding their approach:\\n\\n1. **Problem Definition**: The authors introduce the concept of LLM-based human-agent collaboration for complex task-solving, emphasizing the need for human intuition and wisdom to complement the capabilities of LLMs, especially in dynamic environments.\\n\\n2. **Conditional Automation**: They categorize the collaboration into different levels of automation, where humans intervene only when necessary, allowing LLMs to handle most sub-tasks. This approach is akin to conditional automation in autonomous driving, where human input is required for specific subtasks.\\n\\n3. **ReHAC Method**: The ReHAC method employs a policy model trained using reinforcement learning to determine the optimal moments for human intervention during the task-solving process. This model is designed to balance task performance with the cost of human involvement.\\n\\n4. **Dataset Creation**: The authors constructed a human-agent collaboration dataset to train the policy model in an offline reinforcement learning environment. This dataset consists of tasks that were collaboratively completed by humans and LLM-based agents.\\n\\n5. **Dynamic Decision-Making**: The collaboration is framed as a Markov Decision Process (MDP), where the model dynamically decides whether a human or an agent should take action based on the complexity of the task and the context.\\n\\n6. **Reward Function**: The reward function is designed to maximize task performance while minimizing human intervention costs. It takes into account the expected task rewards and the number of human interventions.\\n\\n7. **Experimental Validation**: The authors conducted experiments on various reasoning datasets to validate the effectiveness of their approach, demonstrating that well-planned, limited human intervention significantly improves performance in complex tasks.\\n\\nOverall, the authors propose a structured framework for human-agent collaboration that leverages the strengths of both humans and LLMs, aiming to enhance the efficiency and effectiveness of complex task-solving.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 417, 'prompt_tokens': 4296, 'total_tokens': 4713, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bba3c8e70b', 'finish_reason': 'stop', 'logprobs': None}, id='run-3daa90c5-5e7d-4e2d-bfc1-4ad8034b94ec-0', usage_metadata={'input_tokens': 4296, 'output_tokens': 417, 'total_tokens': 4713, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "for output in graph_compiled.stream(inputs):\n",
    "    for key,value in output.items():\n",
    "        pprint.pprint(f\"output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value , indent = 2 , width =80 , depth =None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
